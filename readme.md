Win rate per model is proportion of 1:1 wins over other eligible models. Say, there are 25 models in total. For each scenario (e.g., TruthfulQA), each model is pitted against another model (win, loss, tie). Let's say your model won 15 times and lost 9 times. Your model's win rate for this particular scenario is 15/24 (0.62). We have more than one scenario, therefore we need a way to aggregate our win rate across different scenarios. Mean win rate (MWR) calculates the geometric mean of win rates across all scenarios. Tricky part is how you group different tasks under the same scenario, and what aggregation method (e.g., geometric mean) is most appropriate to benchmark models. Just because your model wins against other models does not mean that your model performs well on a task! Hopefully this competition will provide an opportunity to contribute some empirical insights into benchmarking methodology.
OpenLlama-3B. In the Flash Helm paper, authors use an illustrative example where including models that perform slightly less than your model might actually give you an unfair advantage (see attached screenshot below from the paper). Imagine you have a 3B model with the following scores on three tasks: 10, 10, 10. And then you are competing against another model with scores on the same tasks: 12, 12, 8. So you lose on the first two tasks, and win on the last one. You are 1 in 3. Suppose you have a smaller variant of your model (say, 1.5B), and your friend submits this model to the competition. This smaller model performs 9,9,9 on the same tasks. In this case, on the third task, even though you perform worse than the "best" model, because of the new model, you get an additional win on this task that helps inflate your winning record. Depending on the aggregation metric, this inflation and diversity of models can yield to unstable ranking. I think this is particularly problematic when there are multiple variants of good performing models such as Cohere's models so the ranking at the top becomes very sensitive to which models are tested. Getting back to our competition, the toy submission is OpenLlama-3B, and organizers need to set an arbitrary threshold to decide which models to advance to the second evaluation stage. I think they mentioned that your model should at least perform better than the toy model (which serves as a low entry bar to encourage participation and discourage lazy submission). Alternatively, they could also pick some base model such as Mistral 7b as the threshold. ðŸ™‚ I think this will probably be discussed in the conference paper -- how stable the rankings when you choose different thresholds. 
